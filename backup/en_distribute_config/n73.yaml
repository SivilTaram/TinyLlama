train:
  train_data_mixture_en_redpajama: 0.28763399929777694
  train_data_mixture_cleaned_cc100_ind_dedup: 0.22181739895173258
  train_data_mixture_cleaned_cc100_th_dedup: 0.29099303136294974
  train_data_mixture_cleaned_cc100_vi_dedup: 0.19955557038754074
valid:
  valid_data_mixture_en_redpajama: 1.0
  valid_data_mixture_id_wikipedia: 1.0
  valid_data_mixture_th_wikipedia: 1.0
  valid_data_mixture_vi_wikipedia: 1.0
learning_rate: 2.5384031841671203e-05
warmup_steps: 0
total_steps: 10000
decay_lr: false
global_batch_size: 128
model_name: tiny_LLaMA_mistral_120M
