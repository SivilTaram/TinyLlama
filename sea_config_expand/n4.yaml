train:
  train_data_mixture_en_redpajama: 0.10076319639673998
  train_data_mixture_cleaned_cc100_ind_dedup: 0.12424329341573184
  train_data_mixture_cleaned_cc100_lao_dedup: 0.0009232935201539482
  train_data_mixture_cleaned_cc100_ms_dedup: 0.005313672815171623
  train_data_mixture_cleaned_cc100_th_dedup: 0.008180529244100511
  train_data_mixture_cleaned_cc100_vi_dedup: 0.05874950958603621
  train_data_mixture_indonesian_madlad: 0.17788028343942167
  train_data_mixture_malay_madlad: 0.01558693794707392
  train_data_mixture_thai_madlad: 0.081429472421317
  train_data_mixture_vietnamese_madlad: 0.28270312618765414
valid:
  valid_data_mixture_en_redpajama: 1.0
  valid_data_mixture_id_wikipedia: 1.0
  valid_data_mixture_th_wikipedia: 1.0
  valid_data_mixture_vi_wikipedia: 1.0
learning_rate: 0.000010453910747870435
warmup_steps: 0
max_step: 10000
eval_step_interval: 100
save_step_interval: 3000
decay_lr: true
micro_batch_size: 4
global_batch_size: 128
model_name: tiny_LLaMA_1b
